{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "a9c6cf12-7e2e-4517-b6ba-d528c52b9aee"
    }
   },
   "source": [
    "# http://neuralnetworksanddeeplearning.com/chap3.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "6d6e9037-f8ff-4d5b-9188-18213383c6d5"
    }
   },
   "source": [
    "## An improved version of first notebook mnist_numpy, implementing the stochastic gradient descent learning algorithm for a feedforward neural network.  Improvements include the addition of the cross-entropy cost function, regularization, and better initialization of network weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "nbpresent": {
     "id": "7f865238-ab69-431d-a0df-bf969f796af1"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "nbpresent": {
     "id": "66e0f463-cb2a-47e8-bc8a-f7f0afa4c3a4"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "nbpresent": {
     "id": "f4b81b76-b3f0-435e-8d03-790e8a43826f"
    }
   },
   "outputs": [],
   "source": [
    "class CrossEntropyCost(object):\n",
    "    \n",
    "    @staticmethod\n",
    "    def fn(a, y):\n",
    "        \"\"\"Return the cost associated with an output ``a`` and desired output\n",
    "        ``y``.  Note that np.nan_to_num is used to ensure numerical\n",
    "        stability.  In particular, if both ``a`` and ``y`` have a 1.0\n",
    "        in the same slot, then the expression (1-y)*np.log(1-a)\n",
    "        returns nan.  The np.nan_to_num ensures that that is converted\n",
    "        to the correct value (0.0).\"\"\"\n",
    "        return np.sum(np.nan_to_num(-y*np.log(a) - (a-y)*np.log(1-a)))\n",
    "    \n",
    "    @staticmethod\n",
    "    def delta(a, y):\n",
    "        \"\"\"Return the error delta from the output layer.\"\"\"\n",
    "        return a-y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "nbpresent": {
     "id": "57d0703d-5d74-4d9a-a5ea-513658d9f34e"
    }
   },
   "outputs": [],
   "source": [
    "class Network:\n",
    "    \n",
    "    def __init__(self, sizes, cost=CrossEntropyCost):\n",
    "        self.layer_count = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.default_weight_initializer()\n",
    "        self.cost = cost\n",
    "        \n",
    "    def default_weight_initializer(self):\n",
    "        \"\"\"http://neuralnetworksanddeeplearning.com/chap3.html#weight_initialization\n",
    "        \n",
    "        weights are initialized as Gaussian random variables with mean 0 and standard \n",
    "        deviation 1 divided by the square root of the number of connections input to \n",
    "        the neuron.\n",
    "        \n",
    "        biases are initliazed as Gaussian random varibales with mean 0 and standard\n",
    "        deviation 1\n",
    "        \"\"\"\n",
    "        self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)/np.sqrt(x) for y, x in zip(self.sizes[1:], self.sizes[:-1])]\n",
    "        \n",
    "        \n",
    "    def feedforward(self, a):\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            a = sigmoid(np.dot(w, a) + b)\n",
    "        return a\n",
    "    \n",
    "    def SGD(self, train_data, epoch, mini_batch_size, lr, lmbda = 0.0):\n",
    "        \n",
    "        best_accuracy = 0.0\n",
    "        epoch_to_wait = 0\n",
    "        \n",
    "        n = len(train_data)\n",
    "        for i in xrange(epoch):\n",
    "            random.shuffle(train_data)\n",
    "            \n",
    "            mini_batches = [train_data[k:k+mini_batch_size] for k in xrange(0, n, mini_batch_size)]\n",
    "            \n",
    "            for batch in mini_batches:\n",
    "                self.update(batch, lr, lmbda, n)\n",
    "                \n",
    "            print(\"epoch {0} completed\".format(i))\n",
    "            \n",
    "            accuracy = self.accuracy(train_data)/float(n)\n",
    "            print \"Accuracy on training data: {}\".format(accuracy)\n",
    "            \n",
    "            if accuracy <= best_accuracy:\n",
    "                epoch_to_wait += 1\n",
    "                if epoch_to_wait == 10:\n",
    "                    print(\"early stopping...\")\n",
    "                    break\n",
    "            else:\n",
    "                best_accuracy = accuracy\n",
    "                epoch_to_wait = 0\n",
    "            \n",
    "    def update(self, batch, lr, lmbda, n, momentum=1.0):\n",
    "        nebla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nebla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        for x, y in batch:\n",
    "            delta_b, delta_w = self.backprop(x, y)\n",
    "            nebla_b = [nb + db for nb, db in zip(nebla_b, delta_b)]\n",
    "            nebla_w = [nw + dw for nw, dw in zip(nebla_w, delta_w)]\n",
    "            \n",
    "        self.biases = [b - (lr/len(batch))*nb for b, nb in zip(self.biases, nebla_b)]\n",
    "        # L2 regularization\n",
    "        self.weights = [((1-lr*(lmbda/n)*momentum)*w) - (lr/len(batch))*nw for w, nw in zip(self.weights, nebla_w)]\n",
    "    \n",
    "    def backprop(self, x, y):\n",
    "        nebla_w = [np.zeros(w.shape) for w in self.weights] # weight gradient for cost\n",
    "        nebla_b = [np.zeros(b.shape) for b in self.biases] # bias gradient for cost\n",
    "        \n",
    "        # feedforward\n",
    "        act = x\n",
    "        acts = [x] # store all activations, layer-by-layer\n",
    "        zs = [] # store all z vectors, layer-by-layer\n",
    "                \n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, act) + b\n",
    "            zs.append(z)\n",
    "            act = sigmoid(z)\n",
    "            acts.append(act)\n",
    "            \n",
    "        # backward pass\n",
    "        delta = self.cost.delta(acts[-1], y)\n",
    "        \n",
    "        nebla_b[-1] = delta\n",
    "        nebla_w[-1] = np.dot(delta, acts[-2].T)\n",
    "        \n",
    "        for l in xrange(2, self.layer_count):\n",
    "            z = zs[-l]\n",
    "            z_derivative = sigmoid_derivative(z)\n",
    "            delta = np.dot(self.weights[-l+1].T, delta) * z_derivative\n",
    "            nebla_b[-l] = delta\n",
    "            nebla_w[-l] = np.dot(delta, acts[-l-1].T)\n",
    "        return (nebla_b, nebla_w)\n",
    "        \n",
    "    def accuracy(self, data):\n",
    "        results = [(np.argmax(self.feedforward(x)), np.argmax(y)) for (x, y) in data]\n",
    "        return sum(int (x == y) for (x, y) in results)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "nbpresent": {
     "id": "46c794c6-3a2e-4de1-895e-f44f2421b911"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 completed\n",
      "Accuracy on training data: 0.94214\n",
      "epoch 1 completed\n",
      "Accuracy on training data: 0.94474\n",
      "epoch 2 completed\n",
      "Accuracy on training data: 0.95874\n",
      "epoch 3 completed\n",
      "Accuracy on training data: 0.9552\n",
      "epoch 4 completed\n",
      "Accuracy on training data: 0.96018\n",
      "epoch 5 completed\n",
      "Accuracy on training data: 0.96458\n",
      "epoch 6 completed\n",
      "Accuracy on training data: 0.96138\n",
      "epoch 7 completed\n",
      "Accuracy on training data: 0.96496\n",
      "epoch 8 completed\n",
      "Accuracy on training data: 0.96252\n",
      "epoch 9 completed\n",
      "Accuracy on training data: 0.96526\n",
      "epoch 10 completed\n",
      "Accuracy on training data: 0.9649\n",
      "epoch 11 completed\n",
      "Accuracy on training data: 0.94932\n",
      "epoch 12 completed\n",
      "Accuracy on training data: 0.96716\n",
      "epoch 13 completed\n",
      "Accuracy on training data: 0.96158\n",
      "epoch 14 completed\n",
      "Accuracy on training data: 0.9654\n",
      "epoch 15 completed\n",
      "Accuracy on training data: 0.9625\n",
      "epoch 16 completed\n",
      "Accuracy on training data: 0.9641\n",
      "epoch 17 completed\n",
      "Accuracy on training data: 0.96614\n",
      "epoch 18 completed\n",
      "Accuracy on training data: 0.96118\n",
      "epoch 19 completed\n",
      "Accuracy on training data: 0.96912\n",
      "epoch 20 completed\n",
      "Accuracy on training data: 0.9673\n",
      "epoch 21 completed\n",
      "Accuracy on training data: 0.9641\n",
      "epoch 22 completed\n",
      "Accuracy on training data: 0.9602\n",
      "epoch 23 completed\n",
      "Accuracy on training data: 0.96768\n",
      "epoch 24 completed\n",
      "Accuracy on training data: 0.96778\n",
      "epoch 25 completed\n",
      "Accuracy on training data: 0.96854\n",
      "epoch 26 completed\n",
      "Accuracy on training data: 0.96144\n",
      "epoch 27 completed\n",
      "Accuracy on training data: 0.96014\n",
      "epoch 28 completed\n",
      "Accuracy on training data: 0.9674\n",
      "epoch 29 completed\n",
      "Accuracy on training data: 0.96834\n",
      "early stopping...\n"
     ]
    }
   ],
   "source": [
    "import mnist_loader\n",
    "\n",
    "train_data, val_data, test_data = mnist_loader.load_data_wrapper()\n",
    "\n",
    "net = Network([784, 30, 10], cost=CrossEntropyCost)\n",
    "\n",
    "net.SGD(train_data, 30, 10, 0.5, lmbda=5.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
